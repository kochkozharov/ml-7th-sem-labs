# Лабораторные работы по машинному обучению (7 семестр)

## Структура репозитория

```
├── lab1_knn.ipynb          # KNN (K-Nearest Neighbors)
├── lab2_linear.ipynb       # Логистическая и линейная регрессия
├── lab3_tree.ipynb         # Решающее дерево (Decision Tree)
├── lab4_forest.ipynb       # Случайный лес (Random Forest)
├── lab5_boosting.ipynb     # Градиентный бустинг + итоговое сравнение
└── README.md
```

## Датасеты

### Классификация: APS Failure at Scania Trucks
- **Источник**: OpenML (ID: 41138)
- **Задача**: Бинарная классификация — предсказание неисправности системы давления воздуха (APS)
- **Особенности**: 76000 примеров, 170 признаков, несбалансированные классы (~2% positive)
- **Метрики**: F1-score, ROC-AUC

### Регрессия: Avocado Prices
- **Источник**: Kaggle (neuromusic/avocado-prices)
- **Задача**: Предсказание средней цены авокадо
- **Особенности**: ~18000 примеров, 8 признаков
- **Метрики**: RMSE, R²

---

## Итоговое сравнение всех алгоритмов

### Классификация (APS Failure)

| Алгоритм | Бейзлайн F1 | Улучшенный F1 | Бейзлайн ROC-AUC | Улучшенный ROC-AUC |
|----------|-------------|---------------|------------------|-------------------|
| KNN | 0.32 | 0.48 | 0.86 | 0.82 |
| Логистическая регрессия | 0.68 | 0.68 | 0.94 | 0.94 |
| Decision Tree | 0.60 | 0.59 | 0.88 | 0.82 |
| Random Forest | 0.72 | **0.75** | 0.98 | **0.98** |
| Gradient Boosting | 0.62 | 0.68 | 0.98 | 0.98 |

**Лучший алгоритм**: Random Forest (F1=0.75, ROC-AUC=0.98)

### Регрессия (Avocado Prices)

| Алгоритм | Бейзлайн RMSE | Улучшенный RMSE | Бейзлайн R² | Улучшенный R² |
|----------|---------------|-----------------|-------------|---------------|
| KNN | 0.27 | **0.15** | 0.53 | **0.86** |
| Линейная регрессия | 0.31 | 0.31 | 0.39 | 0.39 |
| Decision Tree | 0.28 | 0.21 | 0.52 | 0.73 |
| Random Forest | 0.16 | 0.17 | 0.83 | 0.83 |
| Gradient Boosting | 0.26 | 0.20 | 0.60 | 0.75 |

**Лучший алгоритм**: KNN с масштабированием (R²=0.86, RMSE=0.15)

---

## Собственные реализации алгоритмов

В каждой лабораторной работе реализованы алгоритмы с нуля:

| Лабораторная | Алгоритм | Особенности реализации |
|--------------|----------|------------------------|
| Lab 1 | MyKNN | Евклидово расстояние, взвешивание по расстоянию |
| Lab 2 | MyLinearRegression | Аналитическое решение (псевдообратная матрица) |
| Lab 2 | MyLogisticRegression | Градиентный спуск с L2-регуляризацией |
| Lab 3 | MyDecisionTree | CART (Gini/MSE), рекурсивное построение |
| Lab 4 | MyRandomForest | Бэггинг + случайные подпространства |
| Lab 5 | MyGradientBoosting | Обучение на остатках, log-loss для классификации |

### Сравнение своих реализаций со sklearn

| Алгоритм | sklearn F1 | Своя F1 | sklearn R² | Своя R² |
|----------|------------|---------|------------|---------|
| KNN | 0.32 | 0.15 | 0.53 | 0.71 |
| Линейная/Лог. регрессия | 0.68 | 0.36 | 0.39 | 0.39 |
| Decision Tree | 0.60 | 0.58 | 0.52 | 0.52 |
| Random Forest | 0.72 | 0.61 | 0.83 | 0.55 |
| Gradient Boosting | 0.62 | 0.57 | 0.60 | 0.75 |

---

## Выводы

### По алгоритмам

1. **Random Forest** — лучший алгоритм для классификации (F1=0.75, ROC-AUC=0.98)
   - Устойчив к переобучению благодаря ансамблированию
   - Не требует масштабирования признаков

2. **KNN с масштабированием** — лучший для регрессии Avocado (R²=0.86)
   - Критически важно масштабирование признаков
   - Хорошо работает на данных с локальными паттернами

3. **Логистическая регрессия** — хороший баланс качества и интерпретируемости
   - Быстрое обучение, понятные коэффициенты

4. **Gradient Boosting** — близок к Random Forest, требует настройки learning_rate

5. **Decision Tree** — склонен к переобучению, требует регуляризации (max_depth)

### По улучшениям

- **Масштабирование** (StandardScaler) критично для KNN и линейных моделей
- **Подбор гиперпараметров** (GridSearchCV) даёт +5-50% к качеству
- **Регуляризация** (L2, ограничение глубины) помогает бороться с переобучением

### По собственным реализациям

- Линейная регрессия через псевдообратную матрицу даёт идентичные sklearn результаты
- Деревья и бустинг близки к sklearn при одинаковых параметрах
- KNN и Random Forest отличаются из-за оптимизаций в sklearn (KD-деревья, параллелизм)

---

## Запуск

```bash
# Установка зависимостей
uv sync

# Запуск Jupyter
uv run jupyter lab
```

## Требования

- Python 3.12+
- uv (менеджер пакетов)
- Зависимости: sklearn, pandas, numpy, openml, kagglehub
