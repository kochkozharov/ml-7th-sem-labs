# Прикладные системы и фреймворки искусстевенного интеллекта (7 семестр)

_Кочкожаров Иван Вячеславович М8О-408Б-22_

## Структура репозитория

```
├── lab1_knn.ipynb          # KNN (K-Nearest Neighbors)
├── lab2_linear.ipynb       # Логистическая и линейная регрессия
├── lab3_tree.ipynb         # Решающее дерево (Decision Tree)
├── lab4_forest.ipynb       # Случайный лес (Random Forest)
├── lab5_boosting.ipynb     # Градиентный бустинг + итоговое сравнение
└── README.md
```

## Датасеты

### Классификация: APS Failure at Scania Trucks
- **Источник**: OpenML (ID: 41138)
- **Задача**: Бинарная классификация — предсказание неисправности системы давления воздуха (APS)
- **Особенности**: 76000 примеров, 170 признаков, несбалансированные классы (~2% positive)
- **Метрики**: F1-score, ROC-AUC

### Регрессия: Avocado Prices
- **Источник**: Kaggle (neuromusic/avocado-prices)
- **Задача**: Предсказание средней цены авокадо
- **Особенности**: ~18000 примеров, 8 признаков
- **Метрики**: RMSE, R²

---

## Итоговое сравнение всех алгоритмов

### Классификация (APS Failure)

| Алгоритм | Бейзлайн F1 | Улучшенный F1 | Своя реализация F1 | Лучший ROC-AUC |
|----------|-------------|---------------|-------------------|----------------|
| KNN | 0.32 | 0.48 | 0.33 | 0.90 |
| Логистическая регрессия | 0.68 | 0.68 | 0.58 | 0.97 (своя!) |
| Decision Tree | 0.60 | **0.64** | 0.58 | 0.88 |
| Random Forest | 0.72 | **0.75** | 0.61 | **0.98** |
| Gradient Boosting | 0.62 | 0.68 | 0.57 | 0.98 |

**Лучший алгоритм**: Random Forest (F1=0.75, ROC-AUC=0.98)

### Регрессия (Avocado Prices)

| Алгоритм | Бейзлайн R² | Улучшенный R² | Своя реализация R² | Лучший RMSE |
|----------|-------------|---------------|-------------------|-------------|
| KNN | 0.53 | **0.86** | 0.75 | **0.15** |
| Линейная регрессия | 0.39 | 0.39 | 0.39 | 0.31 |
| Decision Tree | 0.52 | 0.69 | 0.52 | 0.22 |
| Random Forest | 0.83 | 0.84 | 0.83 | 0.16 |
| Gradient Boosting | 0.60 | 0.75 | 0.75 | 0.20 |

**Лучший алгоритм**: KNN с масштабированием (R²=0.86, RMSE=0.15)

---

## Ключевые выводы по улучшениям

### Lab 1 (KNN): Эффект масштабирования

| Этап | F1 (классификация) | R² (регрессия) |
|------|-------------------|----------------|
| Бейзлайн без масштабирования | 0.32 | 0.53 |
| + Масштабирование (StandardScaler) | 0.45 (+0.13) | 0.85 (+0.32) |
| + Подбор гиперпараметров | 0.48 (+0.03) | 0.86 (+0.01) |

**Вывод**: Для KNN масштабирование даёт основной прирост качества (~90% улучшения)

---

## Собственные реализации алгоритмов

| Лабораторная | Алгоритм | Особенности реализации |
|--------------|----------|------------------------|
| Lab 1 | MyKNN | Евклидово расстояние, взвешивание по расстоянию |
| Lab 2 | MyLinearRegression | Аналитическое решение (псевдообратная матрица) |
| Lab 2 | MyLogisticRegression | Градиентный спуск с L2-регуляризацией + оптимальный порог |
| Lab 3 | MyDecisionTree | CART (Gini/MSE), рекурсивное построение |
| Lab 4 | MyRandomForest | Бэггинг + случайные подпространства |
| Lab 5 | MyGradientBoosting | Обучение на остатках, log-loss для классификации |

### Сравнение своих реализаций со sklearn

| Алгоритм | sklearn F1 | Своя F1 | sklearn R² | Своя R² |
|----------|------------|---------|------------|---------|
| KNN | 0.45 | 0.33 | 0.85 | 0.75 |
| Логистическая регрессия | 0.68 | 0.58 | — | — |
| Линейная регрессия | — | — | 0.39 | 0.39 |
| Decision Tree | 0.60 | 0.58 | 0.52 | 0.52 |
| Random Forest | 0.72 | 0.61 | 0.83 | 0.83 |
| Gradient Boosting | 0.62 | 0.57 | 0.60 | 0.75 |

---

## Выводы

### По алгоритмам

1. **Random Forest** — лучший для классификации (F1=0.75, ROC-AUC=0.98)
   - Устойчив к переобучению благодаря ансамблированию
   - Не требует масштабирования признаков

2. **KNN с масштабированием** — лучший для регрессии Avocado (R²=0.86)
   - Критически важно масштабирование признаков
   - Хорошо работает на данных с локальными паттернами

3. **Логистическая регрессия** — хороший баланс качества и скорости
   - Своя реализация показала лучший ROC-AUC=0.97 (выше sklearn!)

4. **Gradient Boosting** — близок к Random Forest, требует настройки learning_rate

5. **Decision Tree** — требует регуляризации (max_depth, min_samples_leaf)

### По улучшениям

- **Масштабирование** (StandardScaler) критично для KNN — даёт +32% к R²
- **Подбор гиперпараметров** (GridSearchCV) даёт дополнительные +3-10%
- **Регуляризация** (L2, ограничение глубины) помогает бороться с переобучением
- **Оптимальный порог** классификации важен для несбалансированных данных

### По собственным реализациям

- Линейная регрессия через псевдообратную матрицу — идентичные sklearn результаты
- Random Forest и Gradient Boosting близки к sklearn при правильных параметрах
- KNN отличается из-за использования подвыборки для ускорения
