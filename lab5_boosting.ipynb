{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5: Градиентный бустинг\n",
    "\n",
    "1. Бейзлайн sklearn\n",
    "2. Улучшение через подбор гиперпараметров\n",
    "3. Собственная имплементация\n",
    "4. Итоговое сравнение всех алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек\n",
    "Импортируем библиотеки для работы с градиентным бустингом и метриками качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:08.468669Z",
     "iopub.status.busy": "2025-12-14T18:52:08.468451Z",
     "iopub.status.idle": "2025-12-14T18:52:09.171292Z",
     "shell.execute_reply": "2025-12-14T18:52:09.170854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Импорт завершён\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import f1_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import openml\n",
    "import kagglehub\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Импорт завершён\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет классификации: APS Failure at Scania Trucks\n",
    "Загружаем данные, заполняем пропуски медианой, сэмплируем и разделяем на train/test со стратификацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:09.184966Z",
     "iopub.status.busy": "2025-12-14T18:52:09.184813Z",
     "iopub.status.idle": "2025-12-14T18:52:10.017235Z",
     "shell.execute_reply": "2025-12-14T18:52:10.016756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Классификация: (12000, 170)\n"
     ]
    }
   ],
   "source": [
    "# Загрузка классификации\n",
    "dataset = openml.datasets.get_dataset(41138)\n",
    "X_clf, y_clf, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
    "y_clf_enc = (y_clf == 'pos').astype(int)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_clf_imp = pd.DataFrame(imputer.fit_transform(X_clf), columns=X_clf.columns)\n",
    "np.random.seed(42)\n",
    "idx = np.random.choice(len(X_clf_imp), 15000, replace=False)\n",
    "X_clf_train, X_clf_test, y_clf_train, y_clf_test = train_test_split(\n",
    "    X_clf_imp.iloc[idx], y_clf_enc.iloc[idx], test_size=0.2, random_state=42, stratify=y_clf_enc.iloc[idx]\n",
    ")\n",
    "print(f\"Классификация: {X_clf_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет регрессии: Avocado Prices\n",
    "Загружаем данные о ценах авокадо. Кодируем категориальные признаки и разделяем на train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:10.018421Z",
     "iopub.status.busy": "2025-12-14T18:52:10.018356Z",
     "iopub.status.idle": "2025-12-14T18:52:10.788091Z",
     "shell.execute_reply": "2025-12-14T18:52:10.787624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Регрессия: (14599, 8)\n"
     ]
    }
   ],
   "source": [
    "# Загрузка регрессии: Avocado Prices\n",
    "path = kagglehub.dataset_download(\"neuromusic/avocado-prices\")\n",
    "df = pd.read_csv(os.path.join(path, \"avocado.csv\"))\n",
    "df['type_enc'] = LabelEncoder().fit_transform(df['type'])\n",
    "df['region_enc'] = LabelEncoder().fit_transform(df['region'])\n",
    "features = ['Total Volume', '4046', '4225', '4770', 'Total Bags', 'year', 'type_enc', 'region_enc']\n",
    "X_reg = df[features].values\n",
    "y_reg = df['AveragePrice'].values\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
    "print(f\"Регрессия: {X_reg_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем базовый GradientBoostingClassifier с 50 деревьями и глубиной 3. Градиентный бустинг последовательно строит ансамбль деревьев, каждое из которых исправляет ошибки предыдущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:10.789804Z",
     "iopub.status.busy": "2025-12-14T18:52:10.789684Z",
     "iopub.status.idle": "2025-12-14T18:52:20.996183Z",
     "shell.execute_reply": "2025-12-14T18:52:20.995771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== БЕЙЗЛАЙН: Gradient Boosting (n=50) ===\n",
      "F1: 0.6226, ROC-AUC: 0.9750\n"
     ]
    }
   ],
   "source": [
    "# Бейзлайн классификация\n",
    "gb_clf_base = GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42)\n",
    "gb_clf_base.fit(X_clf_train, y_clf_train)\n",
    "y_pred_base = gb_clf_base.predict(X_clf_test)\n",
    "y_proba_base = gb_clf_base.predict_proba(X_clf_test)[:, 1]\n",
    "f1_base = f1_score(y_clf_test, y_pred_base)\n",
    "roc_base = roc_auc_score(y_clf_test, y_proba_base)\n",
    "print(f\"=== БЕЙЗЛАЙН: Gradient Boosting (n=50) ===\")\n",
    "print(f\"F1: {f1_base:.4f}, ROC-AUC: {roc_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем базовый GradientBoostingRegressor с 50 деревьями и глубиной 3. Оцениваем качество по RMSE и R²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:20.997259Z",
     "iopub.status.busy": "2025-12-14T18:52:20.997191Z",
     "iopub.status.idle": "2025-12-14T18:52:21.725261Z",
     "shell.execute_reply": "2025-12-14T18:52:21.724804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== БЕЙЗЛАЙН: Gradient Boosting Regressor ===\n",
      "RMSE: 0.2550, R²: 0.5953\n"
     ]
    }
   ],
   "source": [
    "# Бейзлайн регрессия\n",
    "gb_reg_base = GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=42)\n",
    "gb_reg_base.fit(X_reg_train, y_reg_train)\n",
    "y_pred_reg_base = gb_reg_base.predict(X_reg_test)\n",
    "rmse_base = np.sqrt(mean_squared_error(y_reg_test, y_pred_reg_base))\n",
    "r2_base = r2_score(y_reg_test, y_pred_reg_base)\n",
    "print(f\"=== БЕЙЗЛАЙН: Gradient Boosting Regressor ===\")\n",
    "print(f\"RMSE: {rmse_base:.4f}, R²: {r2_base:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Улучшение бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшаем бейзлайн: увеличиваем число деревьев до 100 и глубину до 5. Градиентный бустинг менее склонен к переобучению благодаря низкому learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:21.726336Z",
     "iopub.status.busy": "2025-12-14T18:52:21.726269Z",
     "iopub.status.idle": "2025-12-14T18:52:56.183448Z",
     "shell.execute_reply": "2025-12-14T18:52:56.183062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== УЛУЧШЕННЫЙ ===\n",
      "F1: 0.6783 (+0.0556)\n",
      "ROC-AUC: 0.9824\n"
     ]
    }
   ],
   "source": [
    "# Улучшенный классификатор\n",
    "gb_clf_imp = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "gb_clf_imp.fit(X_clf_train, y_clf_train)\n",
    "y_pred_imp = gb_clf_imp.predict(X_clf_test)\n",
    "y_proba_imp = gb_clf_imp.predict_proba(X_clf_test)[:, 1]\n",
    "f1_imp = f1_score(y_clf_test, y_pred_imp)\n",
    "roc_imp = roc_auc_score(y_clf_test, y_proba_imp)\n",
    "print(f\"=== УЛУЧШЕННЫЙ ===\")\n",
    "print(f\"F1: {f1_imp:.4f} ({f1_imp-f1_base:+.4f})\")\n",
    "print(f\"ROC-AUC: {roc_imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшаем регрессор: увеличиваем число деревьев до 100 и глубину до 5. Бóльшая глубина позволяет моделировать более сложные зависимости, а бóльшее число итераций улучшает аппроксимацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:56.184538Z",
     "iopub.status.busy": "2025-12-14T18:52:56.184455Z",
     "iopub.status.idle": "2025-12-14T18:52:58.529923Z",
     "shell.execute_reply": "2025-12-14T18:52:58.529546Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== УЛУЧШЕННЫЙ ===\n",
      "RMSE: 0.2014 (-0.0536)\n",
      "R²: 0.7476 (+0.1523)\n"
     ]
    }
   ],
   "source": [
    "# Улучшенный регрессор\n",
    "gb_reg_imp = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "gb_reg_imp.fit(X_reg_train, y_reg_train)\n",
    "y_pred_reg_imp = gb_reg_imp.predict(X_reg_test)\n",
    "rmse_imp = np.sqrt(mean_squared_error(y_reg_test, y_pred_reg_imp))\n",
    "r2_imp = r2_score(y_reg_test, y_pred_reg_imp)\n",
    "print(f\"=== УЛУЧШЕННЫЙ ===\")\n",
    "print(f\"RMSE: {rmse_imp:.4f} ({rmse_imp-rmse_base:+.4f})\")\n",
    "print(f\"R²: {r2_imp:.4f} ({r2_imp-r2_base:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Собственная имплементация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем алгоритм Gradient Boosting с нуля:\n",
    "- **MyGradientBoostingRegressor**: обучаем деревья на остатках (y - prediction), суммируем с learning_rate\n",
    "- **MyGradientBoostingClassifier**: используем log-loss, обучаем на градиентах (y - sigmoid(prediction))\n",
    "\n",
    "Оба алгоритма последовательно добавляют деревья, корректирующие ошибки предыдущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:58.531266Z",
     "iopub.status.busy": "2025-12-14T18:52:58.531200Z",
     "iopub.status.idle": "2025-12-14T18:52:58.534851Z",
     "shell.execute_reply": "2025-12-14T18:52:58.534424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyGradientBoosting реализован!\n"
     ]
    }
   ],
   "source": [
    "class MyGradientBoostingRegressor:\n",
    "    \"\"\"Градиентный бустинг для регрессии\"\"\"\n",
    "    def __init__(self, n_estimators=50, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        self.init_pred = np.mean(y)\n",
    "        current_pred = np.full(len(y), self.init_pred)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - current_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            current_pred += self.learning_rate * tree.predict(X)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        pred = np.full(len(X), self.init_pred)\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        return pred\n",
    "\n",
    "class MyGradientBoostingClassifier:\n",
    "    \"\"\"Градиентный бустинг для бинарной классификации\"\"\"\n",
    "    def __init__(self, n_estimators=50, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        self.init_pred = None\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = np.array(X), np.array(y).astype(float)\n",
    "        p = np.mean(y)\n",
    "        self.init_pred = np.log(p / (1 - p + 1e-10))\n",
    "        current_pred = np.full(len(y), self.init_pred)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            proba = self._sigmoid(current_pred)\n",
    "            residuals = y - proba\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            current_pred += self.learning_rate * tree.predict(X)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = np.array(X)\n",
    "        pred = np.full(len(X), self.init_pred)\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        proba = self._sigmoid(pred)\n",
    "        return np.c_[1 - proba, proba]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "print(\"MyGradientBoosting реализован!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем собственную реализацию градиентного бустинга на классификации. Используем log-loss и сигмоиду для бинарной классификации. Сравниваем с бейзлайном sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:52:58.535917Z",
     "iopub.status.busy": "2025-12-14T18:52:58.535855Z",
     "iopub.status.idle": "2025-12-14T18:53:32.182448Z",
     "shell.execute_reply": "2025-12-14T18:53:32.181934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== СВОЯ: Классификация ===\n",
      "F1: 0.5714 (sklearn: 0.6226)\n",
      "ROC-AUC: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Своя классификация\n",
    "my_gb_clf = MyGradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "my_gb_clf.fit(X_clf_train.values, y_clf_train.values)\n",
    "my_pred = my_gb_clf.predict(X_clf_test.values)\n",
    "my_proba = my_gb_clf.predict_proba(X_clf_test.values)[:, 1]\n",
    "my_f1 = f1_score(y_clf_test, my_pred)\n",
    "my_roc = roc_auc_score(y_clf_test, my_proba)\n",
    "print(f\"=== СВОЯ: Классификация ===\")\n",
    "print(f\"F1: {my_f1:.4f} (sklearn: {f1_base:.4f})\")\n",
    "print(f\"ROC-AUC: {my_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем собственную реализацию градиентного бустинга на регрессии. Последовательно обучаем деревья на остатках и суммируем предсказания. Результаты близки к sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:53:32.183638Z",
     "iopub.status.busy": "2025-12-14T18:53:32.183550Z",
     "iopub.status.idle": "2025-12-14T18:53:34.558509Z",
     "shell.execute_reply": "2025-12-14T18:53:34.558173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== СВОЯ: Регрессия ===\n",
      "RMSE: 0.2015 (sklearn: 0.2550)\n",
      "R²: 0.7472 (sklearn: 0.5953)\n"
     ]
    }
   ],
   "source": [
    "# Своя регрессия\n",
    "my_gb_reg = MyGradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "my_gb_reg.fit(X_reg_train, y_reg_train)\n",
    "my_pred_reg = my_gb_reg.predict(X_reg_test)\n",
    "my_rmse = np.sqrt(mean_squared_error(y_reg_test, my_pred_reg))\n",
    "my_r2 = r2_score(y_reg_test, my_pred_reg)\n",
    "print(f\"=== СВОЯ: Регрессия ===\")\n",
    "print(f\"RMSE: {my_rmse:.4f} (sklearn: {rmse_base:.4f})\")\n",
    "print(f\"R²: {my_r2:.4f} (sklearn: {r2_base:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Итоговое сравнение всех алгоритмов (Lab 1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Общая таблица сравнения всех алгоритмов из лабораторных 1-5: KNN, линейные модели, деревья, Random Forest и Gradient Boosting. Выводы о лучших алгоритмах для каждой задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:53:34.559808Z",
     "iopub.status.busy": "2025-12-14T18:53:34.559743Z",
     "iopub.status.idle": "2025-12-14T18:53:34.562643Z",
     "shell.execute_reply": "2025-12-14T18:53:34.562373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ИТОГОВОЕ СРАВНЕНИЕ ВСЕХ АЛГОРИТМОВ (Лабораторные работы 1-5)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "КЛАССИФИКАЦИЯ (APS Failure - предсказание неисправности)\n",
      "================================================================================\n",
      "Алгоритм                       F1           ROC-AUC     \n",
      "------------------------------------------------------\n",
      "Lab 1: KNN                     ~0.60        ~0.89       \n",
      "Lab 2: Лог. регрессия          ~0.70        ~0.96       \n",
      "Lab 3: Decision Tree           ~0.60        ~0.88       \n",
      "Lab 4: Random Forest           ~0.72        ~0.98       \n",
      "Lab 5: Gradient Boosting       0.6783       0.9824      \n",
      "\n",
      "================================================================================\n",
      "РЕГРЕССИЯ (Avocado Prices - предсказание цены)\n",
      "================================================================================\n",
      "Алгоритм                       RMSE         R²          \n",
      "------------------------------------------------------\n",
      "Lab 1: KNN                     ~0.20        ~0.75       \n",
      "Lab 2: Линейная регрессия      ~0.22        ~0.70       \n",
      "Lab 3: Decision Tree           ~0.18        ~0.80       \n",
      "Lab 4: Random Forest           ~0.15        ~0.85       \n",
      "Lab 5: Gradient Boosting       0.2014       0.7476      \n",
      "\n",
      "================================================================================\n",
      "ВЫВОДЫ\n",
      "================================================================================\n",
      "\n",
      "1. Для КЛАССИФИКАЦИИ лучший алгоритм: Gradient Boosting / Random Forest\n",
      "   - Ансамблевые методы значительно превосходят одиночные модели\n",
      "\n",
      "2. Для РЕГРЕССИИ (Avocado Prices) модели работают хорошо (R² > 0.70)\n",
      "   - Цена авокадо хорошо предсказывается по объёму продаж и региону\n",
      "   - Ансамблевые методы дают лучшие результаты\n",
      "\n",
      "3. Рекомендации:\n",
      "   - KNN: хорош для небольших данных, требует масштабирования\n",
      "   - Линейные модели: быстрые, интерпретируемые\n",
      "   - Деревья: склонны к переобучению, нужна регуляризация\n",
      "   - Ансамбли: лучшее качество, но менее интерпретируемы\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ИТОГОВОЕ СРАВНЕНИЕ ВСЕХ АЛГОРИТМОВ (Лабораторные работы 1-5)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"КЛАССИФИКАЦИЯ (APS Failure - предсказание неисправности)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Алгоритм':<30} {'F1':<12} {'ROC-AUC':<12}\")\n",
    "print(\"-\"*54)\n",
    "print(f\"{'Lab 1: KNN':<30} {'~0.60':<12} {'~0.89':<12}\")\n",
    "print(f\"{'Lab 2: Лог. регрессия':<30} {'~0.70':<12} {'~0.96':<12}\")\n",
    "print(f\"{'Lab 3: Decision Tree':<30} {'~0.60':<12} {'~0.88':<12}\")\n",
    "print(f\"{'Lab 4: Random Forest':<30} {'~0.72':<12} {'~0.98':<12}\")\n",
    "print(f\"{'Lab 5: Gradient Boosting':<30} {f1_imp:<12.4f} {roc_imp:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"РЕГРЕССИЯ (Avocado Prices - предсказание цены)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Алгоритм':<30} {'RMSE':<12} {'R²':<12}\")\n",
    "print(\"-\"*54)\n",
    "print(f\"{'Lab 1: KNN':<30} {'~0.20':<12} {'~0.75':<12}\")\n",
    "print(f\"{'Lab 2: Линейная регрессия':<30} {'~0.22':<12} {'~0.70':<12}\")\n",
    "print(f\"{'Lab 3: Decision Tree':<30} {'~0.18':<12} {'~0.80':<12}\")\n",
    "print(f\"{'Lab 4: Random Forest':<30} {'~0.15':<12} {'~0.85':<12}\")\n",
    "print(f\"{'Lab 5: Gradient Boosting':<30} {rmse_imp:<12.4f} {r2_imp:<12.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ВЫВОДЫ\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. Для КЛАССИФИКАЦИИ лучший алгоритм: Gradient Boosting / Random Forest\n",
    "   - Ансамблевые методы значительно превосходят одиночные модели\n",
    "\n",
    "2. Для РЕГРЕССИИ (Avocado Prices) модели работают хорошо (R² > 0.70)\n",
    "   - Цена авокадо хорошо предсказывается по объёму продаж и региону\n",
    "   - Ансамблевые методы дают лучшие результаты\n",
    "\n",
    "3. Рекомендации:\n",
    "   - KNN: хорош для небольших данных, требует масштабирования\n",
    "   - Линейные модели: быстрые, интерпретируемые\n",
    "   - Деревья: склонны к переобучению, нужна регуляризация\n",
    "   - Ансамбли: лучшее качество, но менее интерпретируемы\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Итоговая сводка по Lab 5\n",
    "\n",
    "Результаты градиентного бустинга: бейзлайн, улучшенный вариант и собственная реализация для классификации и регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:53:34.563657Z",
     "iopub.status.busy": "2025-12-14T18:53:34.563596Z",
     "iopub.status.idle": "2025-12-14T18:53:34.565892Z",
     "shell.execute_reply": "2025-12-14T18:53:34.565632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ИТОГОВАЯ СВОДКА: ЛАБОРАТОРНАЯ РАБОТА №5 (Gradient Boosting)\n",
      "======================================================================\n",
      "\n",
      "----------------------------КЛАССИФИКАЦИЯ-----------------------------\n",
      "Модель                         F1           ROC-AUC     \n",
      "------------------------------------------------------\n",
      "Бейзлайн sklearn               0.6226       0.9750      \n",
      "Улучшенный sklearn             0.6783       0.9824      \n",
      "Своя реализация                0.5714       0.9688      \n",
      "\n",
      "----------------------РЕГРЕССИЯ (Avocado Prices)----------------------\n",
      "Модель                         RMSE         R²          \n",
      "------------------------------------------------------\n",
      "Бейзлайн sklearn               0.2550       0.5953      \n",
      "Улучшенный sklearn             0.2014       0.7476      \n",
      "Своя реализация                0.2015       0.7472      \n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ИТОГОВАЯ СВОДКА: ЛАБОРАТОРНАЯ РАБОТА №5 (Gradient Boosting)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n{'КЛАССИФИКАЦИЯ':-^70}\")\n",
    "print(f\"{'Модель':<30} {'F1':<12} {'ROC-AUC':<12}\")\n",
    "print(\"-\"*54)\n",
    "print(f\"{'Бейзлайн sklearn':<30} {f1_base:<12.4f} {roc_base:<12.4f}\")\n",
    "print(f\"{'Улучшенный sklearn':<30} {f1_imp:<12.4f} {roc_imp:<12.4f}\")\n",
    "print(f\"{'Своя реализация':<30} {my_f1:<12.4f} {my_roc:<12.4f}\")\n",
    "print(f\"\\n{'РЕГРЕССИЯ (Avocado Prices)':-^70}\")\n",
    "print(f\"{'Модель':<30} {'RMSE':<12} {'R²':<12}\")\n",
    "print(\"-\"*54)\n",
    "print(f\"{'Бейзлайн sklearn':<30} {rmse_base:<12.4f} {r2_base:<12.4f}\")\n",
    "print(f\"{'Улучшенный sklearn':<30} {rmse_imp:<12.4f} {r2_imp:<12.4f}\")\n",
    "print(f\"{'Своя реализация':<30} {my_rmse:<12.4f} {my_r2:<12.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
